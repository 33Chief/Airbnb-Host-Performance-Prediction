# -*- coding: utf-8 -*-
"""airbnb-host-performance/host_performance_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PYoVjASmTaRD0ts3CEJWTqLzb0nVQ2wy
"""

"""
Airbnb Host Performance Prediction Model
========================================
Author: Malcolm Riley
Date: 2025-10-08

Goal:
-----
This project predicts which Airbnb hosts are most likely to be ‚ÄúGuest Favourites‚Äù
using machine learning. The model analyzes host behavior, listing features,
and performance data to reveal what factors make a host successful.

Models Used:
-------------
- Logistic Regression
- Random Forest Classifier
- XGBoost Classifier

Key Outcome:
-------------
Best model (XGBoost) achieved ROC-AUC of 0.842
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ============================
# 1. LOAD AND EXPLORE DATA
# ============================
print("=" * 60)
print("HOST PERFORMANCE PREDICTION MODEL")
print("=" * 60)

# Load your data (file is in the Jupyter workspace)
df = pd.read_csv('Airbnb_site_hotel new.csv')

print("\nüìä Dataset Overview:")
print(f"Shape: {df.shape}")
print(f"\nColumns: {df.columns.tolist()}")
print(f"\nFirst few rows:")
print(df.head())

# Check target variable distribution
print("\nüéØ Target Variable Distribution (guest favourite):")
print(df['guest favourite'].value_counts())
print(f"Percentage of Guest Favourites: {df['guest favourite'].mean()*100:.2f}%")

# ============================
# 2. DATA PREPROCESSING
# ============================
print("\n" + "=" * 60)
print("DATA PREPROCESSING")
print("=" * 60)

# Create a copy
data = df.copy()

# Check missing values
print("\nüîç Missing Values:")
missing = data.isnull().sum()
missing_pct = (missing / len(data)) * 100
missing_df = pd.DataFrame({
    'Missing Count': missing,
    'Percentage': missing_pct
}).sort_values('Missing Count', ascending=False)
print(missing_df[missing_df['Missing Count'] > 0])

# ============================
# 3. FEATURE ENGINEERING
# ============================
print("\n" + "=" * 60)
print("FEATURE ENGINEERING")
print("=" * 60)

# Convert 'host since' to days since hosting
if 'host since' in data.columns:
    data['host since'] = pd.to_datetime(data['host since'], errors='coerce')
    current_date = pd.to_datetime('today')
    data['days_as_host'] = (current_date - data['host since']).dt.days
    data['years_as_host'] = data['days_as_host'] / 365.25
    print("‚úÖ Created 'days_as_host' and 'years_as_host' from 'host since'")

# Handle 'reply time' if it's categorical (e.g., "within an hour", "within a day")
if 'reply time' in data.columns and data['reply time'].dtype == 'object':
    reply_time_mapping = {
        'within an hour': 1,
        'within a few hours': 2,
        'within a day': 3,
        'a few days or more': 4
    }
    data['reply_time_numeric'] = data['reply time'].map(reply_time_mapping)
    print("‚úÖ Converted 'reply time' to numeric")

# Convert percentage strings to floats if needed (handle European comma format)
for col in ['host response rate', 'host acceptance rate']:
    if col in data.columns and data[col].dtype == 'object':
        data[col] = data[col].str.replace('%', '').str.replace(',', '.').astype(float)
        print(f"‚úÖ Converted '{col}' from percentage string to float")

# Fix numeric columns with comma decimal separator
numeric_cols_with_commas = ['bathrooms', 'price']
for col in numeric_cols_with_commas:
    if col in data.columns and data[col].dtype == 'object':
        data[col] = data[col].str.replace(',', '.').astype(float)
        print(f"‚úÖ Converted '{col}' from comma to decimal format")

# Encode categorical variables
categorical_cols = ['room_type', 'area']
label_encoders = {}

for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[f'{col}_encoded'] = le.fit_transform(data[col].astype(str))
        label_encoders[col] = le
        print(f"‚úÖ Encoded '{col}'")

# ============================
# 4. FEATURE SELECTION
# ============================
print("\n" + "=" * 60)
print("FEATURE SELECTION")
print("=" * 60)

# Define features for modeling
feature_cols = [
    'price',
    'reply_time_numeric' if 'reply_time_numeric' in data.columns else 'reply time',
    'host Certification',
    'host response rate',
    'host acceptance rate',
    'years_as_host' if 'years_as_host' in data.columns else 'days_as_host',
    'consumer total reviewers number',
    'host total listings count',
    'accommodates',
    'bathrooms',
    'bedrooms',
    'beds',
    'listing number',
    'sales',
    'room_type_encoded',
    'area_encoded'
]

# Filter features that exist in the dataset
feature_cols = [col for col in feature_cols if col in data.columns]
print(f"\nüìã Selected Features ({len(feature_cols)}):")
for i, col in enumerate(feature_cols, 1):
    print(f"  {i}. {col}")

# Prepare X and y
X = data[feature_cols].copy()
y = data['guest favourite'].copy()

# Handle missing values
print("\nüîß Handling missing values...")
for col in X.columns:
    if X[col].isnull().sum() > 0:
        if X[col].dtype in ['float64', 'int64']:
            X[col].fillna(X[col].median(), inplace=True)
        else:
            X[col].fillna(X[col].mode()[0], inplace=True)

# Remove any rows with missing target
mask = y.notna()
X = X[mask]
y = y[mask]

print(f"‚úÖ Final dataset shape: {X.shape}")
print(f"‚úÖ Target distribution after cleaning:")
print(y.value_counts())

# ============================
# 5. TRAIN-TEST SPLIT
# ============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nüìä Train set: {X_train.shape}, Test set: {X_test.shape}")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================
# 6. MODEL TRAINING
# ============================
print("\n" + "=" * 60)
print("MODEL TRAINING")
print("=" * 60)

models = {}
results = {}

# ----- Logistic Regression -----
print("\nüîµ Training Logistic Regression...")
lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
lr_model.fit(X_train_scaled, y_train)
models['Logistic Regression'] = lr_model

y_pred_lr = lr_model.predict(X_test_scaled)
y_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]

results['Logistic Regression'] = {
    'predictions': y_pred_lr,
    'probabilities': y_proba_lr,
    'roc_auc': roc_auc_score(y_test, y_proba_lr)
}

print(f"  ‚úÖ ROC-AUC: {results['Logistic Regression']['roc_auc']:.4f}")

# ----- Random Forest -----
print("\nüå≤ Training Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced',
    max_depth=10,
    min_samples_split=10
)
rf_model.fit(X_train, y_train)
models['Random Forest'] = rf_model

y_pred_rf = rf_model.predict(X_test)
y_proba_rf = rf_model.predict_proba(X_test)[:, 1]

results['Random Forest'] = {
    'predictions': y_pred_rf,
    'probabilities': y_proba_rf,
    'roc_auc': roc_auc_score(y_test, y_proba_rf)
}

print(f"  ‚úÖ ROC-AUC: {results['Random Forest']['roc_auc']:.4f}")

# ----- XGBoost -----
print("\n‚ö° Training XGBoost...")
# Calculate scale_pos_weight for imbalanced classes
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    scale_pos_weight=scale_pos_weight,
    eval_metric='logloss'
)
xgb_model.fit(X_train, y_train)
models['XGBoost'] = xgb_model

y_pred_xgb = xgb_model.predict(X_test)
y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]

results['XGBoost'] = {
    'predictions': y_pred_xgb,
    'probabilities': y_proba_xgb,
    'roc_auc': roc_auc_score(y_test, y_proba_xgb)
}

print(f"  ‚úÖ ROC-AUC: {results['XGBoost']['roc_auc']:.4f}")

# ============================
# 7. MODEL EVALUATION
# ============================
print("\n" + "=" * 60)
print("MODEL EVALUATION")
print("=" * 60)

for model_name in ['Logistic Regression', 'Random Forest', 'XGBoost']:
    print(f"\n{'='*50}")
    print(f"üìä {model_name}")
    print(f"{'='*50}")

    y_pred = results[model_name]['predictions']

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Not Favourite', 'Guest Favourite']))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)

# ============================
# 8. VISUALIZATIONS
# ============================
print("\n" + "=" * 60)
print("GENERATING VISUALIZATIONS")
print("=" * 60)

fig = plt.figure(figsize=(18, 12))

# 1. ROC Curves Comparison
ax1 = plt.subplot(2, 3, 1)
for model_name in ['Logistic Regression', 'Random Forest', 'XGBoost']:
    y_proba = results[model_name]['probabilities']
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc_score = results[model_name]['roc_auc']
    ax1.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)

ax1.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
ax1.set_xlabel('False Positive Rate', fontsize=11)
ax1.set_ylabel('True Positive Rate', fontsize=11)
ax1.set_title('ROC Curves Comparison', fontsize=13, fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# 2. Model Performance Comparison
ax2 = plt.subplot(2, 3, 2)
model_names = list(results.keys())
auc_scores = [results[m]['roc_auc'] for m in model_names]
colors = ['#3498db', '#2ecc71', '#e74c3c']
bars = ax2.bar(model_names, auc_scores, color=colors, alpha=0.7, edgecolor='black')
ax2.set_ylabel('ROC-AUC Score', fontsize=11)
ax2.set_title('Model Performance Comparison', fontsize=13, fontweight='bold')
ax2.set_ylim([0.5, 1.0])
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# 3. Random Forest Feature Importance
ax3 = plt.subplot(2, 3, 3)
rf_importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=True).tail(10)

ax3.barh(rf_importances['feature'], rf_importances['importance'], color='#2ecc71', alpha=0.7)
ax3.set_xlabel('Importance', fontsize=11)
ax3.set_title('Top 10 Features (Random Forest)', fontsize=13, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='x')

# 4. XGBoost Feature Importance
ax4 = plt.subplot(2, 3, 4)
xgb_importances = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=True).tail(10)

ax4.barh(xgb_importances['feature'], xgb_importances['importance'], color='#e74c3c', alpha=0.7)
ax4.set_xlabel('Importance', fontsize=11)
ax4.set_title('Top 10 Features (XGBoost)', fontsize=13, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='x')

# 5. Confusion Matrix - Best Model
best_model_name = max(results.keys(), key=lambda k: results[k]['roc_auc'])
ax5 = plt.subplot(2, 3, 5)
cm = confusion_matrix(y_test, results[best_model_name]['predictions'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax5,
            xticklabels=['Not Favourite', 'Guest Favourite'],
            yticklabels=['Not Favourite', 'Guest Favourite'])
ax5.set_xlabel('Predicted', fontsize=11)
ax5.set_ylabel('Actual', fontsize=11)
ax5.set_title(f'Confusion Matrix - {best_model_name}', fontsize=13, fontweight='bold')

# 6. Prediction Distribution
ax6 = plt.subplot(2, 3, 6)
for model_name in ['Logistic Regression', 'Random Forest', 'XGBoost']:
    y_proba = results[model_name]['probabilities']
    ax6.hist(y_proba, bins=30, alpha=0.5, label=model_name, edgecolor='black')

ax6.set_xlabel('Predicted Probability', fontsize=11)
ax6.set_ylabel('Frequency', fontsize=11)
ax6.set_title('Prediction Probability Distribution', fontsize=13, fontweight='bold')
ax6.legend(fontsize=9)
ax6.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('host_performance_analysis.png', dpi=300, bbox_inches='tight')
print("‚úÖ Visualizations saved as 'host_performance_analysis.png'")
plt.show()

# ============================
# 9. FEATURE IMPORTANCE SUMMARY
# ============================
print("\n" + "=" * 60)
print("FEATURE IMPORTANCE SUMMARY")
print("=" * 60)

print("\nüå≤ Random Forest - Top 10 Important Features:")
print(rf_importances.sort_values('importance', ascending=False))

print("\n‚ö° XGBoost - Top 10 Important Features:")
print(xgb_importances.sort_values('importance', ascending=False))

# ============================
# 10. INSIGHTS AND RECOMMENDATIONS
# ============================
print("\n" + "=" * 60)
print("KEY INSIGHTS")
print("=" * 60)

best_model_name = max(results.keys(), key=lambda k: results[k]['roc_auc'])
best_auc = results[best_model_name]['roc_auc']

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   ROC-AUC Score: {best_auc:.4f}")

print("\nüí° What Makes a Guest Favourite Host (Based on Feature Importance):")
top_features = xgb_importances.sort_values('importance', ascending=False).head(5)
for idx, row in top_features.iterrows():
    print(f"   ‚Ä¢ {row['feature']}: {row['importance']:.4f}")

print("\nüìà Recommendations for Host Improvement:")
print("   1. Focus on improving features with highest importance")
print("   2. Maintain quick response times and high acceptance rates")
print("   3. Gather more reviews to build credibility")
print("   4. Consider getting host certification if available")
print("   5. Optimize pricing strategy based on market analysis")

print("\n" + "=" * 60)
print("ANALYSIS COMPLETE!")
print("=" * 60)